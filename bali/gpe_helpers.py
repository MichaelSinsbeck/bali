"""
A collection of small helper-functions for the package.
"""
import numpy as np
from scipy.linalg import cholesky
from scipy.special import gamma, kv

def compute_squared_distances(l, x1, x2):
    return (((x1[:, np.newaxis, :] - x2)/l)**2).sum(2)

def log_mvnpdf(x, m, c):
    xc = x-m
    d = m.size
    const = -0.5*d*np.log(2*np.pi)
    term1 = -0.5 * np.sum(xc @ np.linalg.solve(c, xc))
    term2 = const - 0.5 * log_det(c)
    return term1 + term2

def log_det(A):
    U = cholesky(A)
    return 2*np.sum(np.log(np.diag(U)))

def zero_mean(x, n_output):
    n_sample = x.shape[0]
    return np.zeros([n_sample, n_output])

def covariance_squared_exponential(l, sigma_squared, x1, x2=None):
    if x2 is None:
        x2 = x1
    # l may be a scalar or a vector of size n_input
    squared_distances = compute_squared_distances(l, x1, x2)
    c = np.exp(- squared_distances) * sigma_squared
    
    c += (squared_distances == 0) * sigma_squared * 1e-7
    return c

def covariance_matern(l, sigma_squared, nu, x1, x2=None):
    if x2 is None:
        x2 = x1
        
    n_sample1 = x1.shape[0]
    n_sample2 = x2.shape[0]

    distances = np.sqrt(compute_squared_distances(l, x1, x2))
    d_unique, indices = np.unique(distances, return_inverse=True)
    # np.unique helps speed this up, because special.kv is expensive

    # nugget effect is included here already
    c_unique = np.full_like(d_unique, sigma_squared * (1+1e-7))

    mask = (d_unique != 0)
    C1 = 1 / (gamma(nu) * 2**(nu-1))
    C2 = np.sqrt(2*nu) * d_unique[mask]
    c_unique[mask] = C1 * (C2**nu) * kv(nu, C2) * sigma_squared

    c = c_unique[indices.reshape(n_sample1, n_sample2)]
    return c

def kldiv(ll_true, ll_approx): 
    ll_true = ll_true - np.max(ll_true)
    ll_approx = ll_approx - np.max(ll_approx)
    
    l_true = np.exp(ll_true)
    l_approx = np.exp(ll_approx)
    ll_true = ll_true - np.log(sum(l_true))
    ll_approx = ll_approx - np.log(sum(l_approx))
    
    kldiv = np.dot(np.exp(ll_true), (ll_true-ll_approx))
    return kldiv


def compute_errors(ll_true, ll_approx):
    errors = [kldiv(ll_true, this_ll) for this_ll in ll_approx]
    return errors


def linearize_gpe(gpe_list):
    m = np.mean([gpe.m for gpe in gpe_list], axis=0)
    n_output = m.shape[1]
    n_x = m.shape[0]
    c = np.zeros((n_x, n_x, n_output))
    
    weight = 1./len(gpe_list)
    for i_output in range(n_output):
        for gpe in gpe_list:
            d = gpe.m[:, i_output] - m[:,i_output]
            c[:,:, i_output] += weight * gpe.c
            c[:,:, i_output] += weight * np.outer(d, d)

    return [DiscreteGpe(m,c)]  

def ll_to_lbme(ll):
    ll_shifted = ll - ll.max()
    lbme = np.log(np.mean(np.exp(ll_shifted))) + ll.max()
    return lbme

# Containter for m vector (mean) and c matrices (covariance)
# Is generated by gpe.discretize
class DiscreteGpe:
    def __init__(self, m, c):
        self.m = m
        self.c = c
        self.n_output = m.shape[1]
        self.n_x = m.shape[0]
        
    def draw_realization(self):
        zero_mean = np.zeros(self.n_x)
        realization =  np.random.multivariate_normal(zero_mean, self.c, size = self.n_output).T
        realization += self.m
        return realization
    
    def extract_variance(self):
        c_is_2d = (self.c.ndim == 2)
        if c_is_2d:
            var = np.diag(self.c)[:, np.newaxis]
        else:
            var = np.full((self.n_x, self.n_output), np.nan)
            for i_output in range(self.n_output):
                var[:, i_output] = np.diag(self.c[:,:, i_output])
        return var
    
    def estimate_likelihood(self, data, variance):
        var = self.extract_variance()
        var_nu = var + variance
        
        likelihood = 1./np.sqrt(np.prod(2*np.pi*var_nu, axis=1)) * \
            np.exp(-np.sum((self.m-data)**2/(2*var_nu), axis=1))
            
        return likelihood